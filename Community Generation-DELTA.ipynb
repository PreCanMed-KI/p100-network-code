{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var body = document.body,  \n",
    "    attribs = body.attributes;\n",
    "var command = \"theNotebook = \" + \"'\"+attribs['data-notebook-name'].value+\"'\";\n",
    "kernel.execute(command);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    run_once\n",
    "except NameError:\n",
    "    run_once = False\n",
    "if not run_once:\n",
    "    run_once = True\n",
    "    \n",
    "    import time\n",
    "    import logging\n",
    "    reload(logging)\n",
    "    FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    logpath = '../logs/%s.log' % ('.'.join(theNotebook.split('.')[:-1],))\n",
    "    logging.basicConfig(filename=logpath,level=logging.DEBUG, format=FORMAT)\n",
    "    print(\"logging to %s\" % (logpath))\n",
    "    logger = logging.getLogger()\n",
    "    #logger.basicConfig(filename='/notebooks/Export Microbiome to database.log',level=logging.DEBUG)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "\n",
    "    # create formatter\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # add formatter to ch\n",
    "    ch.setFormatter(formatter)\n",
    "\n",
    "    # add ch to logger\n",
    "    logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas, pandas.io\n",
    "import re\n",
    "import seaborn as sns\n",
    "import math\n",
    "import scipy, scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import networkx as nx\n",
    "import community\n",
    "logging.getLogger('boto').setLevel(logging.INFO)\n",
    "logging.getLogger('p100').setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select(res, substring=None, ids = None):\n",
    "    \"\"\"Helper function to search network dataframe.\n",
    "    \"\"\"\n",
    "    if substring:\n",
    "        ss_sel = _select_substring(res,  substring.lower() )\n",
    "    if ids:\n",
    "        raise Exception(\"id unimplemented\")\n",
    "        id_sel = _select_ids(res, ids[0], ids[1])\n",
    "    return res[ss_sel]\n",
    "\n",
    "def _select_ids( res, datasource_id, variable_id ):\n",
    "    sel = (res.ds_id_1 == datasource_id & res.variable_id_1 == variable_id )\n",
    "    sel = sel | (res.ds_id_2 == datasource_id & res.variable_id_2 == variable_id )\n",
    "    return sel\n",
    "def _select_substring( res, substring ):\n",
    "    \n",
    "    ann = res.annotations_1.unique().tolist() + res.annotations_2.unique().tolist()\n",
    "    sel = res.annotations_1 == ''\n",
    "    text =  set([a for a in ann if a.lower().find(substring) >= 0.0])\n",
    "    for an in text:\n",
    "        sel = sel | (res.annotations_1 == an)\n",
    "        sel = sel | (res.annotations_2 == an) \n",
    "    return sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = pandas.read_pickle('data/community/full.DELTA.correlation.network.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_pval_adj = 0.05\n",
    "\n",
    "sig = res[(res.pval_adj < max_pval_adj)]\n",
    "sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub = select(sig, 'COACH')\n",
    "remove = sub.index\n",
    "mask1 = sig.index.isin(remove)\n",
    "print mask1.sum()\n",
    "sub = select(sig, 'AGES')\n",
    "remove = sub.index\n",
    "mask2 = sig.index.isin(remove)\n",
    "print mask2.sum()\n",
    "sub = select(sig, 'CHEMS.None.Genova.height')\n",
    "remove = sub.index\n",
    "mask3 = sig.index.isin(remove)\n",
    "print mask3.sum()\n",
    "sig = sig[~(mask1 | mask2 | mask3)]\n",
    "sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rem = pandas.read_csv('data/community/duplicate_measurements.05012016.txt', sep='\\t', header=None)\n",
    "rem.columns = ['ann1', 'ann2']\n",
    "\n",
    "mask = []\n",
    "for index, row in sig.iterrows():\n",
    "    \n",
    "    found = False\n",
    "    for index2, row2 in rem.iterrows():\n",
    "        \n",
    "        if (row['annotations_1']==row2['ann1']) and (row['annotations_2']==row2['ann2']):\n",
    "            found = True\n",
    "            break;\n",
    "        elif (row['annotations_1']==row2['ann2']) and (row['annotations_2']==row2['ann1']):\n",
    "            found = True\n",
    "            break;\n",
    "\n",
    "    mask.append(found)\n",
    "    \n",
    "mask = np.array(mask)\n",
    "sig = sig[~mask]\n",
    "sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save resulting network to text file\n",
    "sig.to_pickle('data/community/correlation_network.DELTA.sig.nodups.FULL.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = []\n",
    "# Now remove all correlations between pairwise elements\n",
    "for index, row in sig.iterrows():\n",
    "    \n",
    "    cat1 = row['annotations_1'][0:5]\n",
    "    cat2 = row['annotations_2'][0:5]\n",
    "    \n",
    "    if (cat1 == cat2):\n",
    "        mask.append(False)\n",
    "    else:\n",
    "        mask.append(True)\n",
    "        \n",
    "sig = sig[mask]\n",
    "print sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print select(sig, 'GENOM').shape\n",
    "print select(sig, 'MICRO').shape\n",
    "print select(sig, 'PROTE').shape\n",
    "print select(sig, 'METAB').shape\n",
    "print select(sig, 'CHEMS').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save resulting network to text file\n",
    "sig.to_pickle('data/community/correlation_network.DELTA.sig.nodups.INTRAOMIC.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data into networkx\n",
    "g = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a set of nodes\n",
    "nodes = sig['annotations_1'].append(sig['annotations_2']).unique()\n",
    "datatype = {}\n",
    "super_pathways = {}\n",
    "sub_pathways = {}\n",
    "\n",
    "# Loop over nodes and get type\n",
    "for e in nodes:\n",
    "    tokens = e.split('.')\n",
    "\n",
    "    datatype[e] = tokens[0]\n",
    "    if (datatype[e]=='METAB'):\n",
    "        super_pathways[e] = tokens[2]\n",
    "        sub_pathways[e] = tokens[3]\n",
    "    \n",
    "g.add_nodes_from(list(nodes))\n",
    "nx.set_node_attributes(g, 'type', datatype)\n",
    "nx.set_node_attributes(g, 'super_pathway', super_pathways)\n",
    "nx.set_node_attributes(g, 'sub_pathway', sub_pathways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add a set of edges\n",
    "edges = []\n",
    "attributes = []\n",
    "for row in sig.iterrows():\n",
    "    g.add_edge(row[1]['annotations_1'], row[1]['annotations_2'], {'weight':abs(row[1]['coefficient']), 'non_abs_weight':row[1]['coefficient']})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count number of nodes and edges\n",
    "print g.number_of_nodes(), g.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def girvan_newman(G, k=None, weight=None):\n",
    "    \"\"\"Find communities in graph using Girvan–Newman method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : NetworkX graph\n",
    "\n",
    "    weight : string, optional (default=None)\n",
    "       Edge data key corresponding to the edge weight.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of tuples which contains the clusters of nodes.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> G = nx.path_graph(10)\n",
    "    >>> comp = girvan_newman(G)\n",
    "    >>> comp[0]\n",
    "    ([0, 1, 2, 3, 4], [8, 9, 5, 6, 7])\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The Girvan–Newman algorithm detects communities by progressively removing\n",
    "    edges from the original graph. Algorithm removes edge with the highest\n",
    "    betweenness centrality at each step. As the graph breaks down into pieces,\n",
    "    the tightly knit community structure is exposed and result can be depicted\n",
    "    as a dendrogram.\n",
    "    \"\"\"\n",
    "    # The copy of G here must include the edge weight data.\n",
    "    g = G.copy().to_undirected()\n",
    "    components = []\n",
    "    while g.number_of_edges() > 0:\n",
    "        print \"Number of edges\", g.number_of_edges()\n",
    "        sys.stdout.flush()\n",
    "        _remove_max_edge(g, k, weight)\n",
    "        components.append(tuple(list(H)\n",
    "                                for H in nx.connected_component_subgraphs(g)))\n",
    "    return components\n",
    "\n",
    "def _remove_max_edge(G, k=None, weight=None):\n",
    "    \"\"\"\n",
    "    Removes edge with the highest value on betweenness centrality.\n",
    "\n",
    "    Repeat this step until more connected components than the connected\n",
    "    components of the original graph are detected.\n",
    "\n",
    "    It is part of Girvan–Newman algorithm.\n",
    "\n",
    "    :param G: NetworkX graph\n",
    "    :param weight: string, optional (default=None) Edge data key corresponding\n",
    "    to the edge weight.\n",
    "    \"\"\"\n",
    "    number_components = nx.number_connected_components(G)\n",
    "    #print number_components\n",
    "    while nx.number_connected_components(G) <= number_components:\n",
    "        betweenness = nx.edge_betweenness_centrality(G, k=k, weight=weight)\n",
    "        #print \"finished betweenness\", nx.number_connected_components(G), number_components\n",
    "        max_value = max(betweenness.values())\n",
    "        #print \"max_value\", max_value\n",
    "        # Use a list of edges because G is changed in the loop\n",
    "        for edge in list(G.edges()):\n",
    "            if betweenness[edge] == max_value:\n",
    "                G.remove_edge(*edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note: this takes about an hou\n",
    "communities = girvan_newman(g, weight='weight')\n",
    "\n",
    "# Save the community to a file\n",
    "cindex = 0\n",
    "gindex = 0\n",
    "\n",
    "with open('data/community/generated.communities.DELTA.sig.nodups.INTRAOMIC.txt', 'w') as f:\n",
    "\n",
    "    cindex = 0\n",
    "    for community in communities:\n",
    "        gindex = 0\n",
    "        for graph in community:\n",
    "\n",
    "            for node in graph:\n",
    "                f.write(\"%d\\t%d\\t%s\\n\"%(cindex, gindex, node))\n",
    "\n",
    "            gindex += 1\n",
    "\n",
    "        cindex += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the community\n",
    "communities_dict = []\n",
    "\n",
    "current_community = None\n",
    "current_subgraph = None\n",
    "data = {}\n",
    "\n",
    "# Load the community as a set of dictionaries\n",
    "with open('data/community/generated.communities.DELTA.sig.nodups.INTRAOMIC.txt', 'r') as f:\n",
    "\n",
    "    for line in f:\n",
    "        \n",
    "        tokens = line.strip().split('\\t')\n",
    "        c = int(tokens[0])\n",
    "        subgraph = int(tokens[1])\n",
    "        node = tokens[2]\n",
    "\n",
    "        if (current_community is None):\n",
    "            current_community = c\n",
    "            current_subgraph = subgraph\n",
    "            data[node] = subgraph\n",
    "            \n",
    "        elif (c != current_community):          \n",
    "            communities_dict.append(data)\n",
    "            data = {}\n",
    "            current_community = c\n",
    "            current_subgraph = subgraph\n",
    "            \n",
    "        data[node] = subgraph\n",
    "        \n",
    "    #if (len(data)>0):\n",
    "    #    subgraphs.append(data)\n",
    "        \n",
    "    #if (len(subgraphs)>0):\n",
    "    #    communities_dict.append(subgraphs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import community\n",
    "results = []\n",
    "count = 0\n",
    "mods = []\n",
    "for c in communities_dict:\n",
    "    mod = community.modularity(c, g)\n",
    "    results.append((count, mod))\n",
    "    mods.append(mod)\n",
    "    count += 1\n",
    "    \n",
    "df = pandas.DataFrame(results, columns=['index', 'modularity']).sort_values('modularity', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(mods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the community as a list of lists\n",
    "communities = []\n",
    "\n",
    "current_community = None\n",
    "current_subgraph = None\n",
    "data = []\n",
    "subgraphs = []\n",
    "\n",
    "# Load the community as a set of dictionaries\n",
    "with open('data/community/generated.communities.DELTA.sig.nodups.INTRAOMIC.txt', 'r') as f:\n",
    "\n",
    "    for line in f:\n",
    "        \n",
    "        tokens = line.strip().split('\\t')\n",
    "        c = int(tokens[0])\n",
    "        subgraph = int(tokens[1])\n",
    "        node = tokens[2]\n",
    "\n",
    "        if (current_community is None):\n",
    "            current_community = c\n",
    "            current_subgraph = subgraph\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if (subgraph != current_subgraph):\n",
    "                subgraphs.append(data)\n",
    "                data = []\n",
    "                current_subgraph = subgraph\n",
    "            \n",
    "            if (c != current_community):          \n",
    "                communities.append(subgraphs)\n",
    "                subgraphs = []\n",
    "                current_community = c\n",
    "            \n",
    "        data.append(node)\n",
    "        \n",
    "    if (len(data)>0):\n",
    "        subgraphs.append(data)\n",
    "        \n",
    "    if (len(subgraphs)>0):\n",
    "        communities.append(subgraphs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "COMMUNITY_LEVEL = 20\n",
    "nnodes = []\n",
    "nedges = []\n",
    "for e in communities[COMMUNITY_LEVEL]:\n",
    "    if (len(e)>1):   \n",
    "        sg = g.subgraph(e)\n",
    "        nnodes.append(sg.number_of_nodes())\n",
    "        nedges.append(sg.number_of_edges())\n",
    "        \n",
    "print \"Number of communities\", len(nnodes), len(nedges)\n",
    "print sorted(nnodes)\n",
    "print \"Node stats\", np.mean(nnodes), np.max(nnodes), np.min(nnodes), np.sum(nnodes)\n",
    "print \"Edge stats\", np.mean(nedges), np.max(nedges), np.min(nedges), np.sum(nedges)\n",
    "\n",
    "# Get percentage of nodes used\n",
    "print \"Percent of nodes in a community\", float(np.sum(nnodes)) / g.number_of_nodes()\n",
    "print \"Percent of edges in a community\", float(np.sum(nedges)) / g.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "communities = pandas.read_csv('data/community/generated.communities.DELTA.sig.nodups.INTRAOMIC.txt', header=None, sep='\\t')\n",
    "communities.columns = ['cindex', 'gindex', 'node']\n",
    "communities.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
